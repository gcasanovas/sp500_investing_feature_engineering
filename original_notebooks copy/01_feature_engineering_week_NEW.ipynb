{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to transform data into factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on a conceptual understanding of key factor categories, their rationale and popular metrics, a key task is to identify new factors that may better capture the risks embodied by the return drivers laid out previously, or to find new ones. \n",
    "\n",
    "In either case, it will be important to compare the performance of innovative factors to that of known factors to identify incremental signal gains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the dataset here and store it in our [data](../../data) folder to facilitate reuse in later chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# replaces pyfinance.ols.PandasRollingOLS (no longer maintained)\n",
    "from statsmodels.regression.rolling import RollingOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `assets.h5` store can be generated using the the notebook [create_datasets](../../data/create_datasets.ipynb) in the [data](../../data) directory in the root directory of this repo for instruction to download the following dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the Quandl stock price datasets covering the US equity markets 2000-18 using `pd.IndexSlice` to perform a slice operation on the `pd.MultiIndex`, select the adjusted close price and unpivot the column to convert the DataFrame to wide format with tickers in the columns and timestamps in the rows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set data store location:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otros Settings iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normaliza = True  # normalizamos por volatilidad\n",
    "neutraliza = False  # normalizado cross sectional media y vola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_STORE = \"../data/assets.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = 1990\n",
    "END = 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Carga el dataframe desde el archivo csv\n",
    "df = pd.read_csv('C:/Users/Inma/Documents/mis notebooks/Machine-Learning-for-Algorithmic-Trading-Second-Edition-master/data/tickers_darwinex.csv')\n",
    "\n",
    "columna = df.columns[0]\n",
    "\n",
    "# AÃ±ade '.US' al final de cada valor en la columna\n",
    "df[columna] = df[columna].apply(lambda x: x + '.US')\n",
    "\n",
    "# Almacena los valores de la primera columna en una lista. Los tickers\n",
    "lista = df.iloc[:, 0].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargamos los sectores y otra info\n",
    "df_sect = pd.read_csv('C:/Users/Inma/Documents/mis notebooks/Machine-Learning-for-Algorithmic-Trading-Second-Edition-master/data/us_equities_meta_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sect['ticker'] = df_sect['ticker'].apply(lambda x: x + '.US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sect = df_sect.set_index('ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_sect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sect.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sect.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sect['esta_en_lista'] = df_sect.index.isin(lista)\n",
    "# num_tickers_en_lista = df_sect['esta_en_lista'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_tickers_en_lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# etfs a considerar\n",
    "princi = [\n",
    "    \"XLE.US\",\n",
    "    \"XLB.US\",\n",
    "    \"XLI.US\",\n",
    "    \"XLK.US\",\n",
    "    \"XLF.US\",\n",
    "    \"XLP.US\",\n",
    "    \"XLY.US\",\n",
    "    \"XLV.US\",\n",
    "    \"XLU.US\",\n",
    "    \"IYR.US\",\n",
    "    \"VOX.US\",\n",
    "    \"SPY.US\",\n",
    "]\n",
    "# princi=lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.HDFStore(DATA_STORE) as store:\n",
    "#    tickers = (store['stooq/us/nyse/etfs/prices'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# str(START)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tickers=tickers.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tickers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tickers.index.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    prices1 = (store['stooq/us/nyse/etfs/prices']\n",
    "              .loc[idx[princi, :], :])\n",
    "\n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    prices2 = (store['stooq/us/nyse/stocks/prices']\n",
    "              .loc[idx[princi, :], :])   \n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    prices3 = (store['stooq/us/nasdaq/stocks/prices']\n",
    "              .loc[idx[princi, :], :])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prices = pd.concat([prices1, prices2, prices3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ticker_list = ['XLE', 'XLB', 'XLI', 'XLK', 'XLF', \n",
    "               'XLP', 'XLY', 'XLV', 'XLU', 'IYR', 'VOX', 'SPY']\"\"\"\n",
    "ticker_list = [\"XLE\", \"XLB\", \"XLI\", \"XLK\", \"XLF\", \"XLP\", \"XLY\", \"XLV\", \"XLU\", \"SPY\"]\n",
    "\n",
    "# Here we use yf.download function\n",
    "data = yf.download(\n",
    "    # passes the ticker\n",
    "    tickers=ticker_list,\n",
    "    # used for access data[ticker]\n",
    "    group_by=\"ticker\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apilamos tickers\n",
    "data = data.stack(-2)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.swaplevel(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename_axis([\"ticker\", \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reordenar y renombrar las columnas directamente\n",
    "new_order = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "new_names = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "\n",
    "# Reordenar las columnas\n",
    "prices = data[new_order]\n",
    "\n",
    "# Renombrar las columnas\n",
    "prices.columns = new_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = prices.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un nuevo DataFrame sin entradas duplicadas en el Ã­ndice\n",
    "prices = prices.loc[~prices.index.duplicated(keep=\"first\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select 500 most-traded stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# si queremos selecionar los 500 o no\n",
    "# selec_500= True\n",
    "selec_500 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selec_500 == True:\n",
    "    dv = prices.close.mul(prices.volume)\n",
    "    top500 = (\n",
    "        dv.groupby(level=\"date\")\n",
    "        .rank(ascending=False)\n",
    "        .unstack(\"ticker\")\n",
    "        .dropna(thresh=8 * 52, axis=1)\n",
    "        .mean()\n",
    "        .nsmallest(500)\n",
    "    )\n",
    "    to_drop = prices.index.unique(\"ticker\").difference(top500.index)\n",
    "    len(to_drop)\n",
    "    prices = prices.drop(to_drop, level=\"ticker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prices.index.unique(\"ticker\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponiendo que 'df' es tu DataFrame\n",
    "num_ticker = prices.index.get_level_values(\"ticker\").nunique()\n",
    "\n",
    "print(f'El Ã­ndice \"ticker\" tiene {num_ticker} elementos Ãºnicos.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eliminamos spy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminamos spy\n",
    "prices = prices.drop(index=\"SPY\", level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = prices.sort_index(\n",
    "    level=list(range(len(prices.index.names)))\n",
    ")  # Sort all levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardamos los datos de ohlcv\n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    store.put(\"data_close\", prices.sort_index())\n",
    "    print(store.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacemos unstack de close sÃ³lo\n",
    "prices = prices.loc[idx[:, str(START) : str(END)], \"close\"].unstack(\"ticker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiene_indices_duplicados = prices.index.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiene_indices_duplicados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep data with stock info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove `stocks` duplicates and align index names for later joining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"stocks = stocks[~stocks.index.duplicated()]\n",
    "stocks.index.name = 'ticker'\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get tickers with both price information and metdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"shared = prices.columns.intersection(stocks.index)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stocks = stocks.loc[shared, :]\n",
    "# stocks.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prices = prices.loc[:, shared]\n",
    "# prices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert prices.shape[1] == stocks.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create weekly return series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce training time and experiment with strategies for longer time horizons, we convert the business-daily data to week-end frequency using the available adjusted close price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_prices_real = prices.resample(\n",
    "    \"M\"\n",
    ").last()  # para que las betas de fama french esten alineadas\n",
    "weekly_prices = prices.resample(\"W\").last()\n",
    "# weekly_prices = prices.resample('W-WED').last() #final en miercoles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To capture time series dynamics that reflect, for example, momentum patterns, we compute historical returns using the method `.pct_change(n_periods)`, that is, returns over various weekly periods as identified by lags.\n",
    "\n",
    "We then convert the wide result back to long format with the `.stack()` method, use `.pipe()` to apply the `.clip()` method to the resulting `DataFrame`, and winsorize returns at the [1%, 99%] levels; that is, we cap outliers at these percentiles.\n",
    "\n",
    "Finally, we normalize returns using the geometric average. After using `.swaplevel()` to change the order of the `MultiIndex` levels, we obtain compounded weekly returns for six periods ranging from 1 to 12 weeks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_prices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_cutoff = 0.01\n",
    "data = pd.DataFrame()\n",
    "lags = [1, 2, 3, 6, 12, 52]  # para semanas\n",
    "for lag in lags:\n",
    "    data[f\"return_{lag}w\"] = (\n",
    "        weekly_prices.pct_change(lag)\n",
    "        .stack()\n",
    "        .pipe(\n",
    "            lambda x: x.clip(\n",
    "                lower=x.quantile(outlier_cutoff), upper=x.quantile(1 - outlier_cutoff)\n",
    "            )\n",
    "        )\n",
    "        .add(1)\n",
    "        .pow(1 / lag)\n",
    "        .sub(1)\n",
    "    )\n",
    "data = data.swaplevel().dropna()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para fama frech\n",
    "return_1w_real = weekly_prices_real.pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_1w_real = return_1w_real.stack().swaplevel().dropna()\n",
    "return_1w_real.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_1w_real.name = \"return_1w\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop stocks with less than 10 yrs of returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_obs = 12*10 #mensual\n",
    "min_obs = 52 * 10  # semanal\n",
    "nobs = data.groupby(level=\"ticker\").size()\n",
    "keep = nobs[nobs > min_obs].index\n",
    "\n",
    "data = data.loc[idx[keep, :], :]\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nomalizado de retornos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_by_rolling_std(series):\n",
    "    return series / series.rolling(52).std().shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normaliza=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if normaliza == True:\n",
    "    lags = [1, 2, 3, 6, 12, 52]  # para semanas\n",
    "    for lag in lags:\n",
    "        data[f\"return_{lag}w\"] = data.groupby(level=\"ticker\")[\n",
    "            f\"return_{lag}w\"\n",
    "        ].transform(normalize_by_rolling_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FunciÃ³n para neutralizar (normalizar) los retornos por cada fecha\n",
    "\n",
    "\n",
    "def neutralize(group):\n",
    "    return (group - group.mean()) / group.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neutraliza=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if neutraliza == True:\n",
    "    lags = [1, 2, 3, 6, 12, 52]\n",
    "    for lag in lags:\n",
    "        data[f\"return_{lag}w\"] = data.groupby(level=\"date\")[f\"return_{lag}w\"].transform(\n",
    "            neutralize\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#creamos diferencias de retornos con spy\n",
    "df_SPY = data.loc['SPY.US']\n",
    "new_df = pd.DataFrame()\n",
    "for ticker in data.index.get_level_values(0).unique():\n",
    "    if ticker != 'SPY.US':\n",
    "        df_temp = data.loc[ticker] - df_SPY\n",
    "        df_temp['ticker'] = ticker\n",
    "        new_df = pd.concat([new_df, df_temp])\n",
    "\n",
    "new_df.set_index('ticker', append=True, inplace=True)\n",
    "new_df = new_df.reorder_levels(['ticker', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Si queremos cambiar el target a excess return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=new_df # si queremos cambiar el target a excess retur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index.get_level_values(0).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmap = sns.diverging_palette(10, 220, as_cmap=True)\n",
    "sns.clustermap(data.corr(\"spearman\"), annot=True, center=0, cmap=\"Blues\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are left with 1,670 tickers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index.get_level_values(\"ticker\").nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Factor Betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will introduce the FamaâFrench data to estimate the exposure of assets to common risk factors using linear regression in [Chapter 8, Time Series Models]([](../../08_time_series_models))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The five FamaâFrench factors, namely market risk, size, value, operating profitability, and investment have been shown empirically to explain asset returns and are commonly used to assess the risk/return profile of portfolios. Hence, it is natural to include past factor exposures as financial features in models that aim to predict future returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the historical factor returns using the `pandas-datareader` and estimate historical exposures using the `PandasRollingOLS` rolling linear regression functionality in the `pyfinance` library as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Fama-French research factors to estimate the factor exposures of the stock in the dataset to the 5 factors market risk, size, value, operating profitability and investment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = [\"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\"]\n",
    "factor_data = web.DataReader(\n",
    "    \"F-F_Research_Data_5_Factors_2x3\", \"famafrench\", start=\"2000\"\n",
    ")[0].drop(\"RF\", axis=1)\n",
    "factor_data.index = factor_data.index.to_timestamp()\n",
    "factor_data = factor_data.resample(\"M\").last().ffill().div(100)\n",
    "\n",
    "factor_data.index.name = \"date\"\n",
    "factor_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_data.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importante metemos el retorno sin adaptar para la regresiÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[\"return_1w\"].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_data = factor_data.join(return_1w_real).sort_index()\n",
    "factor_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 24  # 2 years\n",
    "betas = factor_data.groupby(level=\"ticker\", group_keys=False).apply(\n",
    "    lambda x: RollingOLS(\n",
    "        endog=x.return_1w,\n",
    "        exog=sm.add_constant(x.drop(\"return_1w\", axis=1)),\n",
    "        window=min(T, x.shape[0] - 1),\n",
    "    )\n",
    "    .fit(params_only=True)\n",
    "    .params\n",
    "    # .drop('const', axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas.describe().join(betas.sum(1).describe().to_frame(\"total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.diverging_palette(10, 220, as_cmap=True)\n",
    "sns.clustermap(betas.corr(), annot=True, cmap=cmap, center=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# betasc=betas.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# AÃ±ade dos semanas a cada fecha en tu Ã­ndice ya que no estan disponibles en el momento de consulta\n",
    "\n",
    "# ObtÃ©n el nivel 'date' del Ã­ndice\n",
    "dates = betas.index.get_level_values('date')\n",
    "\n",
    "# AÃ±ade dos semanas a cada fecha\n",
    "new_dates = dates + pd.DateOffset(weeks=10)\n",
    "\n",
    "# Crea un nuevo MultiIndex con las nuevas fechas\n",
    "new_index = pd.MultiIndex.from_arrays([betas.index.get_level_values('ticker'), new_dates], names=['ticker', 'date'])\n",
    "\n",
    "# Asigna el nuevo Ã­ndice a tu DataFrame\n",
    "betas.index = new_index\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas.loc[\"XLK\", \"2002\"].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorporamos los cambios en los datos de betas\n",
    "# for columna in betas.columns:\n",
    "#    betas[columna + '_diff'] = betas[columna].diff().replace(0, np.nan).ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.join(\n",
    "    betas.groupby(level=\"ticker\").shift(1)\n",
    ")  # hacemos shift pq lo conoceremos un mes despues\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"const\"].head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute mean for missing factor betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = [\"const\", \"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the result has the correct index structure\n",
    "data[factors] = (\n",
    "    data.groupby(\"ticker\")[factors]\n",
    "    .apply(lambda x: x.ffill())\n",
    "    .reset_index(level=0, drop=True)\n",
    ")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorporamos los cambios en los datos de betas\n",
    "for columna in factors:\n",
    "    data[columna + \"_diff\"] = data[columna].diff().replace(0, np.nan).ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these results to compute momentum factors based on the difference between returns over longer periods and the most recent weekly return, as well as for the difference between 3 and 12 week returns as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lag in [2, 3, 6, 12, 52]:\n",
    "    data[f\"momentum_{lag}\"] = data[f\"return_{lag}w\"].sub(data.return_1w)\n",
    "data[f\"momentum_3_12\"] = data[f\"return_12w\"].sub(data.return_3w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = data.index.get_level_values(\"date\")\n",
    "# data['year'] = dates.year\n",
    "data[\"month\"] = dates.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# machacamos sector con industry que tiene mÃ¡s detalle\n",
    "# df_sect['sector']=df_sect['industry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sector = data.index.get_level_values('ticker')\n",
    "# sec=pd.factorize(sector)[0].astype(int)\n",
    "# data['sector'] = sec\n",
    "# metemos el sector\n",
    "\n",
    "# data= data.join(df_sect['sector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una Serie con el Ã­ndice del DataFrame y los valores del nivel 'ticker'\n",
    "ticker_series = pd.Series(data.index.get_level_values(\"ticker\"), index=data.index)\n",
    "\n",
    "# Usar esta Serie para llenar los valores NA\n",
    "data[\"sector\"] = ticker_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"sector\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"sector\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"sector\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"sector\"].isna()].index.get_level_values(0).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagged returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use lagged values as input variables or features associated with the current observations, we use the .shift() method to move historical returns up to the current period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(1, 7):\n",
    "    data[f\"return_1w_t-{t}\"] = data.groupby(level=\"ticker\").return_1w.shift(t)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target: Holding Period Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, to compute returns for various holding periods, we use the normalized period returns computed previously and shift them back to align them with the current financial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in [1, 2, 3, 6, 12]:\n",
    "    data[f\"target_{t}w\"] = data.groupby(level=\"ticker\")[f\"return_{t}w\"].shift(-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"target_1w\",\n",
    "    \"target_2w\",\n",
    "    \"target_3w\",\n",
    "    \"return_1w\",\n",
    "    \"return_2w\",\n",
    "    \"return_3w\",\n",
    "    \"return_1w_t-1\",\n",
    "    \"return_1w_t-2\",\n",
    "    \"return_1w_t-3\",\n",
    "]\n",
    "\n",
    "data[cols].dropna().sort_index().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create age proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use quintiles of IPO year as a proxy for company age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"data = (data\n",
    "        .join(pd.qcut(stocks.ipoyear, q=5, labels=list(range(1, 6)))\n",
    "              .astype(float)\n",
    "              .fillna(0)\n",
    "              .astype(int)\n",
    "              .to_frame('age')))\n",
    "data.age = data.age.fillna(-1)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dynamic size proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the marketcap information from the NASDAQ ticker info to create a size proxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stocks.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Market cap information is tied to currrent prices. We create an adjustment factor to have the values reflect lower historical prices for each individual stock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"size_factor = (weekly_prices\n",
    "               .loc[data.index.get_level_values('date').unique(),\n",
    "                    data.index.get_level_values('ticker').unique()]\n",
    "               .sort_index(ascending=False)\n",
    "               .pct_change()\n",
    "               .fillna(0)\n",
    "               .add(1)\n",
    "               .cumprod())\n",
    "size_factor.info()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"msize = (size_factor\n",
    "         .mul(stocks\n",
    "              .loc[size_factor.columns, 'marketcap'])).dropna(axis=1, how='all')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Size indicator as deciles per period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute size deciles per week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"data['msize'] = (msize\n",
    "                 .apply(lambda x: pd.qcut(x, q=10, labels=list(range(1, 11)))\n",
    "                        .astype(int), axis=1)\n",
    "                 .stack()\n",
    "                 .swaplevel())\n",
    "data.msize = data.msize.fillna(-1)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"data = data.join(stocks[['sector']])\n",
    "data.sector = data.sector.fillna('Unknown')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data: Recessions & Leading Indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a small and simple dataset so we can focus on the workflow. We use the Federal Reserveâs Economic Data (FRED) service (see Chapter 2) to download the US recession dates as defined by the National Bureau of Economic Research. We also source four variables that are commonly used to predict the onset of a recession (Kelley 2019) and available via FRED, namely:\n",
    "\n",
    "The long-term spread of the treasury yield curve, defined as the difference between the ten-year and the three-week Treasury yield.\n",
    "The University of Michiganâs consumer sentiment indicator\n",
    "The National Financial Conditions Index (NFCI), and\n",
    "The NFCI nonfinancial leverage subindex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download from FRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = [\n",
    "    \"JHDUSRGDPBR\",\n",
    "    \"T10Y3M\",\n",
    "    \"BAMLC0A0CM\",\n",
    "    \"BAMLH0A0HYM2\",\n",
    "    \"BAMLHE00EHYIOAS\",\n",
    "    \"UMCSENT\",\n",
    "    \"UNRATE\",\n",
    "    \"GDPC1\",\n",
    "    \"DCOILWTICO\",\n",
    "    \"CORESTICKM159SFRBATL\",\n",
    "    \"USSLIND\",\n",
    "    \"VIXCLS\",\n",
    "    \"OVXCLS\",\n",
    "    \"ICSA\",\n",
    "    \"MARTSMPCSM44000USS\",\n",
    "    \"RSXFS\",\n",
    "    \"TREAST\",\n",
    "    \"DGS1\",\n",
    "]\n",
    "var_names = [\n",
    "    \"recession\",\n",
    "    \"yield_curve\",\n",
    "    \"corp_oas\",\n",
    "    \"hy_oas\",\n",
    "    \"eu_hy_oas\",\n",
    "    \"sentiment\",\n",
    "    \"empleo\",\n",
    "    \"real_gdp\",\n",
    "    \"oil\",\n",
    "    \"inflacion\",\n",
    "    \"leading\",\n",
    "    \"vix\",\n",
    "    \"vixoil\",\n",
    "    \"weekjobclaims\",\n",
    "    \"retail_sales_percent\",\n",
    "    \"retail_sales\",\n",
    "    \"us_asset_balance\",\n",
    "    \"1y_yield\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = var_names[1:]\n",
    "label = var_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_display = [\n",
    "    \"Recession\",\n",
    "    \"Yield Curve\",\n",
    "    \"corp_oas\",\n",
    "    \"hy_oas\",\n",
    "    \"eu_hy_oas\",\n",
    "    \"Sentiment\",\n",
    "    \"empleo\",\n",
    "    \"real_gdp\",\n",
    "    \"oil\",\n",
    "    \"inflacion\",\n",
    "    \"leading\",\n",
    "    \"vix\",\n",
    "    \"vixoil\",\n",
    "    \"weekjobclaims\",\n",
    "    \"retail_sales_percent\",\n",
    "    \"retail_sales\",\n",
    "    \"us_asset_balance\",\n",
    "    \"1y_yield\",\n",
    "]\n",
    "col_dict = dict(zip(var_names, var_display))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fred = (\n",
    "    web.DataReader(indicators, \"fred\", 1980, END + 1).resample(\"W\").last().ffill()\n",
    ")\n",
    "data_fred.columns = var_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fred.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We standardize the features so all have mean 0 standard deviation of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_fred.loc[:, features] = scale(data_fred.loc[:, features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fred.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fred.index.name = \"date\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# AÃ±ade  semanas a cada fecha en tu Ã­ndice ya que no estan disponibles en el momento de consulta\n",
    "\n",
    "# AÃ±ade dos semanas a cada fecha en tu Ã­ndice\n",
    "data_fred.index = data_fred.index + pd.DateOffset(weeks=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fred.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_fred = data_fred.drop(data_fred.index[-1]) #eliminamos el Ãºltimo registro que es repetido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorporamos los cambios en los datos\n",
    "for columna in data_fred.columns:\n",
    "    data_fred[columna + \"_diff\"] = data_fred[columna].diff().replace(0, np.nan).ffill()\n",
    "    data_fred[columna + \"_chg\"] = (\n",
    "        data_fred[columna].pct_change().replace(0, np.nan).ffill()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminamos algunas variables que tienen mucha dependencia del nivel historico\n",
    "data_fred = data_fred.drop([\"empleo\", \"us_asset_balance\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para hacer bbfill sÃ³lo hasta que encuentre un primer valor\n",
    "for columna in data_fred.columns:\n",
    "    # Verificar si la columna tiene NaN al inicio\n",
    "    if data_fred[columna].isna().iloc[0]:\n",
    "        # Obtiene el primer valor no NaN de la columna\n",
    "        primer_valor = data_fred[columna].dropna().iloc[0]\n",
    "        # Rellena los NaN iniciales con el primer valor no NaN\n",
    "        data_fred[columna][: data_fred[columna].first_valid_index()] = primer_valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fred.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_fred.index = data_fred.index.to_timestamp()\n",
    "data_fred.index.name = \"date\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.join(data_fred)\n",
    "\n",
    "# Data final shape\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the data again in several later chapters, starting in [Chapter 7 on Linear Models](../../07_linear_models/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    store.put(\"engineered_features\", data.sort_index())\n",
    "    store.put(\"data_raw\", data2.sort_index())  # antes de normalizado de retornos\n",
    "    print(store.info())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230.355px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
